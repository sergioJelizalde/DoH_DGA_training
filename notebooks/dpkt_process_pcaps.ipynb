{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06f6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpkt\n",
    "import socket\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/home/ubuntu/DoH_DGA_training/datasets/DGA\")\n",
    "pcap_files = list(base_dir.glob(\"*.pcap\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989c3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inet_to_str(inet):\n",
    "    try:\n",
    "        return socket.inet_ntop(socket.AF_INET, inet)\n",
    "    except ValueError:\n",
    "        return socket.inet_ntop(socket.AF_INET6, inet)\n",
    "\n",
    "def normalize_flow(src, dst, sport, dport, proto):\n",
    "    if (src, sport) < (dst, dport):\n",
    "        return (src, dst, sport, dport, proto), \"forward\"\n",
    "    else:\n",
    "        return (dst, src, dport, sport, proto), \"reverse\"\n",
    "\n",
    "def process_pcap(pcap_file, max_packets=None):\n",
    "    flows = defaultdict(lambda: {\n",
    "        \"sent_timestamps\": [], \"recv_timestamps\": [],\n",
    "        \"sent_sizes\": [], \"recv_sizes\": []\n",
    "    })\n",
    "\n",
    "    with open(pcap_file, \"rb\") as f:\n",
    "        pcap = dpkt.pcap.Reader(f)\n",
    "\n",
    "        for i, (ts, buf) in enumerate(pcap):\n",
    "            try:\n",
    "                eth = dpkt.ethernet.Ethernet(buf)\n",
    "                if not isinstance(eth.data, dpkt.ip.IP):\n",
    "                    continue\n",
    "                ip = eth.data\n",
    "                l4 = ip.data\n",
    "                if not isinstance(l4, (dpkt.tcp.TCP, dpkt.udp.UDP)):\n",
    "                    continue\n",
    "\n",
    "                proto = \"TCP\" if isinstance(l4, dpkt.tcp.TCP) else \"UDP\"\n",
    "                src = inet_to_str(ip.src)\n",
    "                dst = inet_to_str(ip.dst)\n",
    "                sport = l4.sport\n",
    "                dport = l4.dport\n",
    "\n",
    "                fid, direction = normalize_flow(src, dst, sport, dport, proto)\n",
    "\n",
    "                if direction == \"forward\":\n",
    "                    flows[fid][\"sent_timestamps\"].append(float(ts))\n",
    "                    flows[fid][\"sent_sizes\"].append(len(ip))\n",
    "                else:\n",
    "                    flows[fid][\"recv_timestamps\"].append(float(ts))\n",
    "                    flows[fid][\"recv_sizes\"].append(len(ip))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if max_packets and i > max_packets:\n",
    "                break\n",
    "\n",
    "    # Aggregate features\n",
    "    records = []\n",
    "    for fid, stats in flows.items():\n",
    "        sent_ts = sorted(stats[\"sent_timestamps\"])\n",
    "        recv_ts = sorted(stats[\"recv_timestamps\"])\n",
    "        sent_sizes = stats[\"sent_sizes\"]\n",
    "        recv_sizes = stats[\"recv_sizes\"]\n",
    "\n",
    "        sent_iats = np.diff(sent_ts) if len(sent_ts) > 1 else []\n",
    "        recv_iats = np.diff(recv_ts) if len(recv_ts) > 1 else []\n",
    "\n",
    "        server_delay = None\n",
    "        if sent_ts and recv_ts:\n",
    "            server_delay = max(0.0, recv_ts[0] - sent_ts[0])\n",
    "\n",
    "        records.append({\n",
    "            \"flow_id\": fid,\n",
    "            \"src_ip\": fid[0],\n",
    "            \"dst_ip\": fid[1],\n",
    "            \"src_port\": fid[2],\n",
    "            \"dst_port\": fid[3],\n",
    "            \"protocol\": fid[4],\n",
    "\n",
    "            \"n_sent\": len(sent_sizes),\n",
    "            \"sent_bytes\": int(np.sum(sent_sizes)) if sent_sizes else 0,\n",
    "            \"sent_pkt_min\": int(np.min(sent_sizes)) if sent_sizes else 0,\n",
    "            \"sent_pkt_mean\": float(np.mean(sent_sizes)) if sent_sizes else 0.0,\n",
    "            \"sent_pkt_max\": int(np.max(sent_sizes)) if sent_sizes else 0,\n",
    "            \"sent_iat_min\": float(np.min(sent_iats)) if len(sent_iats) else 0.0,\n",
    "            \"sent_iat_mean\": float(np.mean(sent_iats)) if len(sent_iats) else 0.0,\n",
    "            \"sent_iat_max\": float(np.max(sent_iats)) if len(sent_iats) else 0.0,\n",
    "\n",
    "            \"n_recv\": len(recv_sizes),\n",
    "            \"recv_bytes\": int(np.sum(recv_sizes)) if recv_sizes else 0,\n",
    "            \"recv_pkt_min\": int(np.min(recv_sizes)) if recv_sizes else 0,\n",
    "            \"recv_pkt_mean\": float(np.mean(recv_sizes)) if recv_sizes else 0.0,\n",
    "            \"recv_pkt_max\": int(np.max(recv_sizes)) if recv_sizes else 0,\n",
    "            \"recv_iat_min\": float(np.min(recv_iats)) if len(recv_iats) else 0.0,\n",
    "            \"recv_iat_mean\": float(np.mean(recv_iats)) if len(recv_iats) else 0.0,\n",
    "            \"recv_iat_max\": float(np.max(recv_iats)) if len(recv_iats) else 0.0,\n",
    "\n",
    "            \"server_delay\": server_delay if server_delay is not None else 0.0,\n",
    "            \"bytes_ratio\": float(np.sum(sent_sizes)) / np.sum(recv_sizes)\n",
    "                if recv_sizes and np.sum(recv_sizes) > 0 else 0.0\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7436ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing padcrypt-doh-24h.pcap...\n",
      "Loaded 216 flows with label=padcrypt-doh-24h\n",
      "Processing sisron-doh-24h.pcap...\n",
      "Loaded 38 flows with label=sisron-doh-24h\n",
      "Processing zloader-doh-24h.pcap...\n",
      "Loaded 295 flows with label=zloader-doh-24h\n",
      "Processing tinba-doh-24h.pcap...\n",
      "Loaded 1707 flows with label=tinba-doh-24h\n",
      "Saved aggregated dataframe with 2256 flows across 4 labels\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Processing {pcap_file.name}...\")\n",
    "    df = process_pcap(pcap_file)\n",
    "\n",
    "    # Extract label (everything before the first underscore)\n",
    "    label = pcap_file.stem.split(\"_\")[0]\n",
    "    df[\"label\"] = label\n",
    "\n",
    "    all_dfs.append(df)\n",
    "    print(f\"Loaded {df.shape[0]} flows with label={label}\")\n",
    "\n",
    "# Concatenate everything into one dataframe\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Save once\n",
    "final_df.to_csv(\"/home/ubuntu/DoH_DGA_training/datasets/DGA/all_pcaps.csv\", index=False)\n",
    "print(f\"Saved aggregated dataframe with {final_df.shape[0]} flows across {final_df['label'].nunique()} labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0abb9647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2256, 25)\n"
     ]
    }
   ],
   "source": [
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125aea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dnstt-48h-2021-11-03-08_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-08\n",
      "Processing dnstt-48h-2021-11-03-01_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-01\n",
      "Processing dnstt-48h-2021-11-01-11_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-11\n",
      "Processing dnstt-48h-2021-11-02-22_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-22\n",
      "Processing dnstt-48h-2021-11-01-19_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-19\n",
      "Processing dnstt-48h-2021-11-02-03_46_36.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-03\n",
      "Processing dnstt-48h-2021-11-03-02_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-02\n",
      "Processing dnstt-48h-2021-11-01-12_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-12\n",
      "Processing dnstt-48h-2021-11-02-13_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-13\n",
      "Processing dnstt-48h-2021-11-02-14_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-14\n",
      "Processing dnstt-48h-2021-11-02-04_46_36.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-04\n",
      "Processing dnstt-48h-2021-11-02-05_46_36.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-05\n",
      "Processing dnstt-48h-2021-11-02-20_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-20\n",
      "Processing dnstt-48h-2021-11-02-17_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-17\n",
      "Processing dnstt-48h-2021-11-02-00_46_34.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-00\n",
      "Processing dnstt-48h-2021-11-03-00_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-00\n",
      "Processing dnstt-48h-2021-11-03-06_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-06\n",
      "Processing dnstt-48h-2021-11-02-10_01_45.pcap...\n",
      "Loaded 24 flows with label=dnstt-48h-2021-11-02-10\n",
      "Processing dnstt-48h-2021-11-01-09_46_32.pcap...\n",
      "Loaded 24 flows with label=dnstt-48h-2021-11-01-09\n",
      "Processing dnstt-48h-2021-11-02-02_46_35.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-02\n",
      "Processing dnstt-48h-2021-11-03-03_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-03\n",
      "Processing dnstt-48h-2021-11-01-20_46_34.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-20\n",
      "Processing dnstt-48h-2021-11-01-13_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-13\n",
      "Processing dnstt-48h-2021-11-02-21_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-21\n",
      "Processing dnstt-48h-2021-11-01-15_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-15\n",
      "Processing dnstt-48h-2021-11-02-07_46_36.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-07\n",
      "Processing dnstt-48h-2021-11-01-22_46_34.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-22\n",
      "Processing dnstt-48h-2021-11-03-05_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-05\n",
      "Processing dnstt-48h-2021-11-01-14_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-14\n",
      "Processing dnstt-48h-2021-11-02-01_46_35.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-01\n",
      "Processing dnstt-48h-2021-11-02-15_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-15\n",
      "Processing dnstt-48h-2021-11-01-18_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-18\n",
      "Processing dnstt-48h-2021-11-01-10_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-10\n",
      "Processing dnstt-48h-2021-11-02-12_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-12\n",
      "Processing dnstt-48h-2021-11-03-10_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-10\n",
      "Processing dnstt-48h-2021-11-03-07_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-07\n",
      "Processing dnstt-48h-2021-11-02-06_46_36.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-06\n",
      "Processing dnstt-48h-2021-11-01-21_46_34.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-21\n",
      "Processing dnstt-48h-2021-11-02-16_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-16\n",
      "Processing dnstt-48h-2021-11-02-23_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-23\n",
      "Processing dnstt-48h-2021-11-02-19_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-19\n",
      "Processing dnstt-48h-2021-11-01-23_46_34.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-23\n",
      "Processing dnstt-48h-2021-11-01-16_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-16\n",
      "Processing dnstt-48h-2021-11-03-04_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-04\n",
      "Processing dnstt-48h-2021-11-02-18_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-18\n",
      "Processing dnstt-48h-2021-11-02-11_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-02-11\n",
      "Processing dnstt-48h-2021-11-03-09_01_45.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-03-09\n",
      "Processing dnstt-48h-2021-11-01-17_46_33.pcap...\n",
      "Loaded 25 flows with label=dnstt-48h-2021-11-01-17\n",
      "Saved aggregated dataframe with 1198 flows across 48 labels\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_dir = Path(\"/home/ubuntu/DoH_DGA_training/datasets/DoH_HKD/DoH-Pcaps/DoH-Pcaps-dnstt\")\n",
    "pcap_files = list(base_dir.glob(\"*.pcap\"))\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    print(f\"Processing {pcap_file.name}...\")\n",
    "    df = process_pcap(pcap_file)\n",
    "\n",
    "    # Extract label (everything before the first underscore)\n",
    "    label = pcap_file.stem.split(\"_\")[0]\n",
    "    df[\"label\"] = label\n",
    "\n",
    "    all_dfs.append(df)\n",
    "    print(f\"Loaded {df.shape[0]} flows with label={label}\")\n",
    "\n",
    "# Concatenate everything into one dataframe\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Save once\n",
    "final_df.to_csv(\"/home/ubuntu/DoH_DGA_training/datasets/DoH_HKD/DoH-Pcaps/DoH-Pcaps-dnstt/dnstt_all_pcaps.csv\", index=False)\n",
    "print(f\"Saved aggregated dataframe with {final_df.shape[0]} flows across {final_df['label'].nunique()} labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61eed74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
