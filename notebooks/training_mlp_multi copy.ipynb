{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28da30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGA_shape: (415820, 35)\n",
      "DoH_Tunnel shape: (702676, 35)\n",
      "NonDoH_Benign shape: (2139204, 36)\n",
      "DoH_Benign shape: (116412, 36)\n",
      "\n",
      "Final dataset shape: (3490524, 36)\n",
      "Labels distribution:\n",
      " label\n",
      "NonDoH_Benign    2139204\n",
      "DoH_Tunnel        702676\n",
      "DGA               415820\n",
      "DoH_Benign        232824\n",
      "Name: count, dtype: int64\n",
      "Columns:\n",
      " ['session_id', 'client_ip', 'server_ip', 'client_port', 'server_port', 'protocol', 'N', 'n_client', 'client_bytes', 'client_pkt_min', 'client_pkt_mean', 'client_pkt_max', 'client_iat_min', 'client_iat_mean', 'client_iat_max', 'n_server', 'server_bytes', 'server_pkt_min', 'server_pkt_mean', 'server_pkt_max', 'server_iat_min', 'server_iat_mean', 'server_iat_max', 'pkt_fraction_client', 'bytes_fraction_client', 'flow_duration', 'time_first_response', 'dir_switches', 'size_min', 'size_mean', 'size_max', 'iat_min', 'iat_mean', 'iat_max', 'label', 'source_pcap']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ===============================\n",
    "# File paths\n",
    "# ===============================\n",
    "\n",
    "doh_tunnel_iodine = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHMalicious/iodine/all_pcaps_allN.csv\"\n",
    "doh_tunnel_dns2tcp = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHMalicious/dns2tcp/all_pcaps_allN.csv\"\n",
    "doh_tunnel_dnscat2 = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHMalicious/dnscat2/all_pcaps_allN.csv\"\n",
    "\n",
    "#doh_tunnel_hkd = \"/home/ubuntu/DoH_DGA_training/datasets/DoH_HKD/DoH-Pcaps/all_pcaps_allN.csv\"\n",
    "\n",
    "dga_malware_google = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_Google/all_pcaps_allN.csv\"\n",
    "dga_malware_clouflare = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_CF/all_pcaps_allN.csv\"\n",
    "dga_malware_adguard = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_ADGuard/all_pcaps_allN.csv\"\n",
    "dga_malware_quad9 = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_Quad9/all_pcaps_allN.csv\"\n",
    "\n",
    "dga_malware_hkd = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_HKD/all_pcaps_allN.csv\"\n",
    "\n",
    "nondoh_benign_file = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/all_nondoh.csv\"\n",
    "\n",
    "doh_benign_file = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/all_doh.csv\"\n",
    "\n",
    "# ===============================\n",
    "# Load datasets\n",
    "# ===============================\n",
    "df_doh_tunnel_iodine = pd.read_csv(doh_tunnel_iodine)\n",
    "df_doh_tunnel_dns2tcp = pd.read_csv(doh_tunnel_dns2tcp)\n",
    "df_doh_tunnel_dnscat2 = pd.read_csv(doh_tunnel_dnscat2)\n",
    "\n",
    "#df_doh_tunnel_hkd = pd.read_csv(doh_tunnel_hkd)\n",
    "\n",
    "df_doh_tunnel = pd.concat([df_doh_tunnel_iodine, df_doh_tunnel_dns2tcp, df_doh_tunnel_dnscat2], ignore_index=True)\n",
    "\n",
    "df_dga_google = pd.read_csv(dga_malware_google)\n",
    "df_dga_clouflare = pd.read_csv(dga_malware_clouflare)\n",
    "df_dga_adguard = pd.read_csv(dga_malware_adguard)\n",
    "df_dga_quad9 = pd.read_csv(dga_malware_quad9)\n",
    "df_dga_hkd = pd.read_csv(dga_malware_hkd)\n",
    "df_dga = pd.concat([df_dga_google, df_dga_clouflare, df_dga_adguard, df_dga_quad9, df_dga_hkd], ignore_index=True)\n",
    "\n",
    "df_nondoh_benign = pd.read_csv(nondoh_benign_file)\n",
    "df_doh_benign = pd.read_csv(doh_benign_file)\n",
    "\n",
    "# ===============================\n",
    "# Assign labels\n",
    "# ===============================\n",
    "df_dga[\"label\"] = \"DGA\"\n",
    "df_doh_tunnel[\"label\"] = \"DoH_Tunnel\"\n",
    "df_nondoh_benign[\"label\"] = \"NonDoH_Benign\"\n",
    "df_doh_benign[\"label\"] = \"DoH_Benign\"\n",
    "\n",
    "# ===============================\n",
    "# Combine all into one DataFrame\n",
    "# ===============================\n",
    "df_all = pd.concat(\n",
    "    [df_dga, df_doh_tunnel, df_nondoh_benign, df_doh_benign, df_doh_benign],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"DGA_shape:\", df_dga.shape)\n",
    "print(\"DoH_Tunnel shape:\", df_doh_tunnel.shape)\n",
    "print(\"NonDoH_Benign shape:\", df_nondoh_benign.shape)\n",
    "print(\"DoH_Benign shape:\", df_doh_benign.shape)\n",
    "\n",
    "print(\"\\nFinal dataset shape:\", df_all.shape)\n",
    "print(\"Labels distribution:\\n\", df_all[\"label\"].value_counts())\n",
    "print(\"Columns:\\n\", list(df_all.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d64c40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on N=16, dataset size=872631\n",
      "Selected 16 top features: ['client_pkt_max', 'n_client', 'bytes_fraction_client', 'n_server', 'pkt_fraction_client', 'client_bytes', 'server_pkt_max', 'size_min', 'size_mean', 'server_pkt_mean', 'dir_switches', 'server_bytes', 'size_max', 'client_pkt_min', 'server_pkt_min', 'client_pkt_mean']\n",
      "Remaining feature columns:\n",
      " ['client_pkt_max', 'n_client', 'bytes_fraction_client', 'n_server', 'pkt_fraction_client', 'client_bytes', 'server_pkt_max', 'size_min', 'size_mean', 'server_pkt_mean', 'dir_switches', 'server_bytes', 'size_max', 'client_pkt_min', 'server_pkt_min', 'client_pkt_mean']\n",
      "Target classes: ['DGA' 'DoH_Tunnel' 'NonDoH_Benign' 'DoH_Benign']\n"
     ]
    }
   ],
   "source": [
    "N_value = 16   # <-- change this to 8, 16, 32, 64 as needed\n",
    "df_subset = df_all[df_all[\"N\"] == N_value].copy()\n",
    "top_features = [\n",
    "    \"client_pkt_max\",\n",
    "    \"n_client\",\n",
    "    \"bytes_fraction_client\",\n",
    "    \"n_server\",\n",
    "    \"pkt_fraction_client\",\n",
    "    \"client_bytes\",\n",
    "    \"server_pkt_max\",\n",
    "    \"size_min\",\n",
    "    \"size_mean\",\n",
    "    \"server_pkt_mean\",\n",
    "    \"dir_switches\",\n",
    "    \"server_bytes\",\n",
    "    \"size_max\",\n",
    "    \"client_pkt_min\",\n",
    "    \"server_pkt_min\",\n",
    "    \"client_pkt_mean\"\n",
    "]\n",
    "\n",
    "print(f\"\\nTraining on N={N_value}, dataset size={df_subset.shape[0]}\")\n",
    "\n",
    "# Separate features and target\n",
    "y = df_subset[\"label\"]\n",
    "X = df_subset.drop(columns=[\n",
    "    \"label\", 'session_id', 'client_ip', 'server_ip', \n",
    "    'client_port', 'server_port', 'protocol', 'N',\n",
    "    'pcap_file', 'source_pcap'\n",
    "], errors=\"ignore\")\n",
    "feature_names = X.columns.tolist()\n",
    "# Keep only numeric features\n",
    "X = X.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "\n",
    "# Keep only the predefined top features (in the exact order)\n",
    "available_features = [f for f in top_features if f in X.columns]\n",
    "missing_features = [f for f in top_features if f not in X.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(\"Warning: Missing features:\", missing_features)\n",
    "\n",
    "X = X[available_features].copy()\n",
    "print(f\"Selected {len(available_features)} top features: {available_features}\")\n",
    "\n",
    "print(\"Remaining feature columns:\\n\", list(X.columns))\n",
    "print(\"Target classes:\", y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9566ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.3, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.06138728\n",
      "Validation score: 0.990538\n",
      "Iteration 2, loss = 0.03127899\n",
      "Validation score: 0.994107\n",
      "Iteration 3, loss = 0.02687173\n",
      "Validation score: 0.990931\n",
      "Iteration 4, loss = 0.02408647\n",
      "Validation score: 0.991062\n",
      "Iteration 5, loss = 0.02250228\n",
      "Validation score: 0.996038\n",
      "Iteration 6, loss = 0.02113137\n",
      "Validation score: 0.995547\n",
      "Iteration 7, loss = 0.01802479\n",
      "Validation score: 0.996087\n",
      "Iteration 8, loss = 0.01843537\n",
      "Validation score: 0.992748\n",
      "Iteration 9, loss = 0.01740346\n",
      "Validation score: 0.996955\n",
      "Iteration 10, loss = 0.01711664\n",
      "Validation score: 0.993435\n",
      "Iteration 11, loss = 0.01565428\n",
      "Validation score: 0.995940\n",
      "Iteration 12, loss = 0.01444269\n",
      "Validation score: 0.997413\n",
      "Iteration 13, loss = 0.01522190\n",
      "Validation score: 0.993665\n",
      "Iteration 14, loss = 0.01524454\n",
      "Validation score: 0.996890\n",
      "Iteration 15, loss = 0.01461721\n",
      "Validation score: 0.997397\n",
      "Iteration 16, loss = 0.01529560\n",
      "Validation score: 0.997512\n",
      "Iteration 17, loss = 0.01366326\n",
      "Validation score: 0.997348\n",
      "Iteration 18, loss = 0.01377760\n",
      "Validation score: 0.997168\n",
      "Iteration 19, loss = 0.01329766\n",
      "Validation score: 0.997184\n",
      "Iteration 20, loss = 0.01296245\n",
      "Validation score: 0.993403\n",
      "Iteration 21, loss = 0.01406468\n",
      "Validation score: 0.997332\n",
      "Iteration 22, loss = 0.01487260\n",
      "Validation score: 0.995547\n",
      "Iteration 23, loss = 0.01389564\n",
      "Validation score: 0.996186\n",
      "Iteration 24, loss = 0.01489643\n",
      "Validation score: 0.993321\n",
      "Iteration 25, loss = 0.01192231\n",
      "Validation score: 0.979586\n",
      "Iteration 26, loss = 0.01166941\n",
      "Validation score: 0.997790\n",
      "Iteration 27, loss = 0.01220516\n",
      "Validation score: 0.993615\n",
      "Iteration 28, loss = 0.01222489\n",
      "Validation score: 0.997643\n",
      "Iteration 29, loss = 0.01200745\n",
      "Validation score: 0.997282\n",
      "Iteration 30, loss = 0.01162904\n",
      "Validation score: 0.997806\n",
      "Iteration 31, loss = 0.01307602\n",
      "Validation score: 0.996382\n",
      "Iteration 32, loss = 0.01239586\n",
      "Validation score: 0.997855\n",
      "Iteration 33, loss = 0.01199181\n",
      "Validation score: 0.997708\n",
      "Iteration 34, loss = 0.01074384\n",
      "Validation score: 0.997643\n",
      "Iteration 35, loss = 0.01105666\n",
      "Validation score: 0.997626\n",
      "Iteration 36, loss = 0.01209979\n",
      "Validation score: 0.997708\n",
      "Iteration 37, loss = 0.01137030\n",
      "Validation score: 0.997299\n",
      "Iteration 38, loss = 0.01253716\n",
      "Validation score: 0.997708\n",
      "Iteration 39, loss = 0.01302158\n",
      "Validation score: 0.997364\n",
      "Iteration 40, loss = 0.01242565\n",
      "Validation score: 0.993632\n",
      "Iteration 41, loss = 0.01161626\n",
      "Validation score: 0.998036\n",
      "Iteration 42, loss = 0.01093612\n",
      "Validation score: 0.997528\n",
      "Iteration 43, loss = 0.01290374\n",
      "Validation score: 0.997561\n",
      "Iteration 44, loss = 0.01091326\n",
      "Validation score: 0.998003\n",
      "Iteration 45, loss = 0.01025053\n",
      "Validation score: 0.997332\n",
      "Iteration 46, loss = 0.01078038\n",
      "Validation score: 0.997905\n",
      "Iteration 47, loss = 0.01171260\n",
      "Validation score: 0.997741\n",
      "Iteration 48, loss = 0.01153433\n",
      "Validation score: 0.998183\n",
      "Iteration 49, loss = 0.01050869\n",
      "Validation score: 0.997954\n",
      "Iteration 50, loss = 0.01083111\n",
      "Validation score: 0.997070\n",
      "Iteration 51, loss = 0.01030113\n",
      "Validation score: 0.997610\n",
      "Iteration 52, loss = 0.01141303\n",
      "Validation score: 0.996153\n",
      "Iteration 53, loss = 0.01072845\n",
      "Validation score: 0.997872\n",
      "Iteration 54, loss = 0.00949023\n",
      "Validation score: 0.997872\n",
      "Iteration 55, loss = 0.00969837\n",
      "Validation score: 0.994221\n",
      "Iteration 56, loss = 0.01021554\n",
      "Validation score: 0.997823\n",
      "Iteration 57, loss = 0.00963516\n",
      "Validation score: 0.997839\n",
      "Iteration 58, loss = 0.00965691\n",
      "Validation score: 0.997741\n",
      "Iteration 59, loss = 0.01043914\n",
      "Validation score: 0.997757\n",
      "Iteration 60, loss = 0.01002372\n",
      "Validation score: 0.997986\n",
      "Iteration 61, loss = 0.00992352\n",
      "Validation score: 0.996366\n",
      "Iteration 62, loss = 0.01051442\n",
      "Validation score: 0.971826\n",
      "Iteration 63, loss = 0.01007120\n",
      "Validation score: 0.997741\n",
      "Iteration 64, loss = 0.01033628\n",
      "Validation score: 0.997675\n",
      "Iteration 65, loss = 0.00912636\n",
      "Validation score: 0.997332\n",
      "Iteration 66, loss = 0.00967439\n",
      "Validation score: 0.997806\n",
      "Iteration 67, loss = 0.01042584\n",
      "Validation score: 0.997937\n",
      "Iteration 68, loss = 0.01030384\n",
      "Validation score: 0.998150\n",
      "Iteration 69, loss = 0.00970279\n",
      "Validation score: 0.997855\n",
      "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DGA     0.9996    0.9994    0.9995     31186\n",
      "   DoH_Benign     0.9857    0.9948    0.9902     17462\n",
      "   DoH_Tunnel     0.9989    0.9980    0.9984     52701\n",
      "NonDoH_Benign     0.9989    0.9982    0.9986    160441\n",
      "\n",
      "     accuracy                         0.9981    261790\n",
      "    macro avg     0.9958    0.9976    0.9967    261790\n",
      " weighted avg     0.9981    0.9981    0.9981    261790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train MLP\n",
    "# Extended MLP with more options\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(32, 16),   # deeper network with 3 hidden layers\n",
    "    activation=\"relu\",                  # alternatives: 'tanh', 'logistic'\n",
    "    solver=\"adam\",                      # alternatives: 'sgd', 'lbfgs'\n",
    "    alpha=1e-4,                         # L2 regularization (weight decay)\n",
    "    batch_size=64,                      # mini-batch size\n",
    "    learning_rate=\"adaptive\",           # 'constant', 'invscaling', 'adaptive'\n",
    "    learning_rate_init=0.001,           # initial learning rate\n",
    "    max_iter=200,                       # train longer\n",
    "    shuffle=True,                       # shuffle samples every epoch\n",
    "    early_stopping=True,                # stop if validation score doesn’t improve\n",
    "    validation_fraction=0.1,            # use 10% of training for validation\n",
    "    n_iter_no_change=20,                # patience for early stopping\n",
    "    random_state=42,                    # reproducibility\n",
    "    verbose=True                        # print progress during training\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09cc4c",
   "metadata": {},
   "source": [
    "# Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "315be046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote weights to ./mlp_multi_models/mlp128_weights.h\n",
      "[OK] Wrote scaler stats to ./mlp_multi_models/feature128_stats.h\n",
      "[OK] Wrote feature names to ./mlp_multi_models/feature_names.txt\n",
      "\n",
      "=== Sanity ===\n",
      "Feature count: 16\n",
      "Layer sizes  : [16, 128, 64, 4]\n",
      "Layer 0: W (16, 128), B (128,)\n",
      "Layer 1: W (128, 64), B (64,)\n",
      "Layer 2: W (64, 4), B (4,)\n",
      "\n",
      "All done — check folder: ./mlp_multi_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "output_folder = \"./mlp_multi_models\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Adjust these to match your environment ----\n",
    "# 1) If you used a pipeline similar to your example, set pipe = results[\"pipeline\"]\n",
    "pipe = None               # <-- set this if you have a pipeline object\n",
    "# 2) If you trained the MLP directly, ensure mlp_reduced is available in the namespace\n",
    "mlp = mlp                # <-- will be set below\n",
    "# 3) Provide the scaler if you used it standalone, or let the script extract it from the pipeline\n",
    "scaler = scaler            # <-- set to your StandardScaler instance if not using a pipeline\n",
    "# 4) Provide your feature list (order must match training columns used)\n",
    "#    If you used top_features when fitting, set feature_cols = top_features\n",
    "feature_cols = top_features       # <-- e.g. feature_cols = list(top_features) or results[\"feature_cols\"]\n",
    "\n",
    "# ----------------- Helper: write arrays to C -----------------\n",
    "def write_array_to_c(name, array, f, add_f_suffix=False):\n",
    "    shape = array.shape\n",
    "    flat  = array.flatten()\n",
    "    f.write(f\"// Shape: {shape}\\n\")\n",
    "    f.write(f\"static const float {name}[{len(flat)}] ALIGN16 = {{\\n    \")\n",
    "    if add_f_suffix:\n",
    "        f.write(\", \".join(f\"{float(x):.6f}f\" for x in flat))\n",
    "    else:\n",
    "        f.write(\", \".join(f\"{float(x):.6f}\" for x in flat))\n",
    "    f.write(\"\\n};\\n\\n\")\n",
    "\n",
    "def export_mlp_to_c(model, out_path):\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"// Auto-generated MLP weights + pointers for C inference\\n\")\n",
    "        f.write(\"// Generated from scikit-learn MLPClassifier\\n\\n\")\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(\"#define ALIGN16 __attribute__((aligned(16)))\\n\\n\")\n",
    "\n",
    "        n_layers = len(model.coefs_)  # number of weight matrices (hidden layers + output)\n",
    "        f.write(f\"#define NUM_LAYERS {n_layers}\\n\\n\")\n",
    "        # number of outputs = neurons in final layer\n",
    "        output_size = model.coefs_[-1].shape[1]\n",
    "\n",
    "        # if binary classifier (single output neuron) -> 2 classes\n",
    "        num_classes = 2 if output_size == 1 else output_size\n",
    "\n",
    "        f.write(f\"#define NUM_CLASSES {num_classes}\\n\\n\")\n",
    "        in_size = model.coefs_[0].shape[0]\n",
    "        sizes   = [in_size] + [w.shape[1] for w in model.coefs_]\n",
    "        f.write(\"static const int LAYER_SIZES[NUM_LAYERS+1] = { \" +\n",
    "                \", \".join(str(int(s)) for s in sizes) +\n",
    "                \" };\\n\\n\")\n",
    "\n",
    "        # determine output size (number of outputs from final layer)\n",
    "        output_size = int(model.coefs_[-1].shape[1])\n",
    "        f.write(f\"#define OUTPUT_SIZE {output_size}\\n\\n\")\n",
    "\n",
    "        # Write conditional compile flags for binary vs multiclass\n",
    "        f.write(\"// Convenience macros for selecting code paths at compile time\\n\")\n",
    "        f.write(\"#if OUTPUT_SIZE == 1\\n\")\n",
    "        f.write(\"    #define IS_BINARY_CLASSIFICATION 1\\n\")\n",
    "        f.write(\"    #define IS_MULTICLASS_CLASSIFICATION 0\\n\")\n",
    "        f.write(\"#else\\n\")\n",
    "        f.write(\"    #define IS_BINARY_CLASSIFICATION 0\\n\")\n",
    "        f.write(\"    #define IS_MULTICLASS_CLASSIFICATION 1\\n\")\n",
    "        f.write(\"#endif\\n\\n\")\n",
    "\n",
    "        for idx, (W, b) in enumerate(zip(model.coefs_, model.intercepts_)):\n",
    "            # sklearn W is (size_in, size_out)\n",
    "            W = W.astype(np.float32, copy=False)\n",
    "            B = b.astype(np.float32, copy=False)\n",
    "            write_array_to_c(f\"W{idx}\", W, f)\n",
    "            write_array_to_c(f\"B{idx}\", B, f)\n",
    "\n",
    "        f.write(\"// Weight & bias pointers per layer\\n\")\n",
    "        f.write(\"static const float *WEIGHTS[NUM_LAYERS] = { \" +\n",
    "                \", \".join(f\"W{j}\" for j in range(n_layers)) +\n",
    "                \" };\\n\")\n",
    "        f.write(\"static const float *BIASES[NUM_LAYERS]  = { \" +\n",
    "                \", \".join(f\"B{j}\" for j in range(n_layers)) +\n",
    "                \" };\\n\\n\")\n",
    "\n",
    "        f.write(\"#undef ALIGN16\\n\")\n",
    "\n",
    "# ----------------- Determine mlp, scaler, feature names -----------------\n",
    "# Case A: user provides a pipeline object in variable `pipe`\n",
    "if pipe is not None:\n",
    "    try:\n",
    "        mlp = pipe.named_steps[\"clf\"]\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"Pipeline provided but 'clf' not found in named_steps\")\n",
    "\n",
    "    # try to locate scaler inside a ColumnTransformer inside preproc\n",
    "    try:\n",
    "        preproc = pipe.named_steps[\"preproc\"]\n",
    "        # assumes ColumnTransformer named transformer 'num' containing a pipeline with scaler step named 'scaler'\n",
    "        scaler = preproc.named_transformers_[\"num\"].named_steps[\"scaler\"]\n",
    "    except Exception:\n",
    "        # fallback: look for any StandardScaler in pipeline\n",
    "        for name, step in preproc.named_steps.items():\n",
    "            if isinstance(step, StandardScaler):\n",
    "                scaler = step\n",
    "                break\n",
    "\n",
    "# Case B: pipeline not used, user trained `mlp_reduced` directly\n",
    "if mlp is None:\n",
    "    # try to use provided mlp_reduced from user's scope\n",
    "    try:\n",
    "        mlp = globals().get(\"mlp_reduced\", None) or globals().get(\"mlp\", None)\n",
    "    except Exception:\n",
    "        mlp = None\n",
    "\n",
    "if mlp is None:\n",
    "    raise RuntimeError(\"Could not locate trained MLP. Make sure `mlp_reduced` or `pipe` is defined.\")\n",
    "\n",
    "# If scaler not found, maybe user passed a plain StandardScaler variable; if not, attempt to compute mean/std from training data if available\n",
    "if scaler is None:\n",
    "    # try to find StandardScaler in globals\n",
    "    scaler = globals().get(\"scaler\", None)\n",
    "\n",
    "# If still None, but feature columns & X_train are available, compute scaler from training data (this reproduces what you'd do for inference stats)\n",
    "if scaler is None:\n",
    "    X_train = globals().get(\"X_train\", None)\n",
    "    if X_train is not None and feature_cols is not None:\n",
    "        print(\"[WARN] No StandardScaler instance found — computing mean/std from X_train[feature_cols]. This must match the scaler used at training time.\")\n",
    "        tmp_scaler = StandardScaler()\n",
    "        tmp_scaler.fit(X_train[feature_cols])\n",
    "        scaler = tmp_scaler\n",
    "    else:\n",
    "        raise RuntimeError(\"No scaler found and cannot compute it (X_train or feature_cols missing). Provide the scaler or set X_train/feature_cols.\")\n",
    "\n",
    "# If feature_cols not provided and you have a pipeline with feature names available in e.g. results dict\n",
    "if feature_cols is None:\n",
    "    # try common names used in examples\n",
    "    feature_cols = globals().get(\"feature_cols\", None) or globals().get(\"top_features\", None)\n",
    "    if feature_cols is None:\n",
    "        # try to infer from scaler if it's a named transformer (rare)\n",
    "        raise RuntimeError(\"feature_cols is not set. Provide the ordered list of features used to train the model.\")\n",
    "\n",
    "# ----------------- Compute means/stds and safe-guard -----------------\n",
    "means = scaler.mean_.astype(np.float32)\n",
    "stds  = scaler.scale_.astype(np.float32)\n",
    "\n",
    "# safety to avoid divide-by-zero in C inference\n",
    "stds[stds == 0.0] = 1.0\n",
    "\n",
    "# ----------------- Export MLP weights -----------------\n",
    "mlp_header_path = os.path.join(output_folder, \"mlp128_weights.h\")\n",
    "export_mlp_to_c(mlp, mlp_header_path)\n",
    "print(f\"[OK] Wrote weights to {mlp_header_path}\")\n",
    "\n",
    "# ----------------- Export feature stats header -----------------\n",
    "feat_header_path = os.path.join(output_folder, \"feature128_stats.h\")\n",
    "with open(feat_header_path, \"w\") as f:\n",
    "    f.write(\"// Auto-generated feature stats for z-score normalization\\n\\n\")\n",
    "    f.write(\"#pragma once\\n\\n\")\n",
    "    f.write(\"#define ALIGN16 __attribute__((aligned(16)))\\n\\n\")\n",
    "    f.write(f\"#define NUM_FEATURES {len(means)}\\n\\n\")\n",
    "\n",
    "    f.write(\"static const float FEATURE_MEAN[NUM_FEATURES] ALIGN16 = {\\n    \")\n",
    "    f.write(\", \".join(f\"{float(m):.6f}f\" for m in means))\n",
    "    f.write(\"\\n};\\n\\n\")\n",
    "\n",
    "    f.write(\"static const float FEATURE_STD[NUM_FEATURES] ALIGN16 = {\\n    \")\n",
    "    f.write(\", \".join(f\"{float(s):.6f}f\" for s in stds))\n",
    "    f.write(\"\\n};\\n\\n\")\n",
    "    f.write(\"#undef ALIGN16\\n\")\n",
    "print(f\"[OK] Wrote scaler stats to {feat_header_path}\")\n",
    "\n",
    "# ----------------- Save feature order for verification -----------------\n",
    "names_txt = os.path.join(output_folder, \"feature_names.txt\")\n",
    "with open(names_txt, \"w\") as f:\n",
    "    for c in feature_cols:\n",
    "        f.write(str(c) + \"\\n\")\n",
    "print(f\"[OK] Wrote feature names to {names_txt}\")\n",
    "\n",
    "# ----------------- Sanity print -----------------\n",
    "print(\"\\n=== Sanity ===\")\n",
    "print(\"Feature count:\", len(feature_cols))\n",
    "layer_sizes = [mlp.coefs_[0].shape[0]] + [w.shape[1] for w in mlp.coefs_]\n",
    "print(\"Layer sizes  :\", layer_sizes)\n",
    "for i, (W, b) in enumerate(zip(mlp.coefs_, mlp.intercepts_)):\n",
    "    print(f\"Layer {i}: W {W.shape}, B {b.shape}\")\n",
    "\n",
    "print(\"\\nAll done — check folder:\", output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df52e7",
   "metadata": {},
   "source": [
    "## Debugging flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba26837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder class mapping:\n",
      "  0 → DGA\n",
      "  1 → DoH_Benign\n",
      "  2 → DoH_Tunnel\n",
      "  3 → NonDoH_Benign\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"LabelEncoder class mapping:\")\n",
    "for i, c in enumerate(le.classes_):\n",
    "    print(f\"  {i} → {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3a68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
