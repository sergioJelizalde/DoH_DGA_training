{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28da30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGA_MALWARE shape: (9012, 35)\n",
      "DGA_Synthetic shape: (203428, 35)\n",
      "DoH_Tunnel shape: (6008, 36)\n",
      "NonDoH_Benign shape: (2139204, 36)\n",
      "DoH_Benign shape: (116412, 36)\n",
      "\n",
      "Final dataset shape: (2474064, 37)\n",
      "Labels distribution:\n",
      " label\n",
      "NonDoH_Benign    2139204\n",
      "DGA_Synthetic     203428\n",
      "DoH_Benign        116412\n",
      "DGA_MALWARE         9012\n",
      "DoH_Tunnel          6008\n",
      "Name: count, dtype: int64\n",
      "Columns:\n",
      " ['session_id', 'client_ip', 'server_ip', 'client_port', 'server_port', 'protocol', 'N', 'n_client', 'client_bytes', 'client_pkt_min', 'client_pkt_mean', 'client_pkt_max', 'client_iat_min', 'client_iat_mean', 'client_iat_max', 'n_server', 'server_bytes', 'server_pkt_min', 'server_pkt_mean', 'server_pkt_max', 'server_iat_min', 'server_iat_mean', 'server_iat_max', 'pkt_fraction_client', 'bytes_fraction_client', 'flow_duration', 'time_first_response', 'dir_switches', 'size_min', 'size_mean', 'size_max', 'iat_min', 'iat_mean', 'iat_max', 'label', 'pcap_file', 'source_pcap']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ===============================\n",
    "# File paths\n",
    "# ===============================\n",
    "dga_malware_file = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_HKD/all_pcaps_allN.csv\"\n",
    "doh_tunnel_file = \"/home/ubuntu/DoH_DGA_training/datasets/DoH_HKD/DoH-Pcaps/all_pcaps_allN.csv\"\n",
    "dga_malware_synthetic1 = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_Google/all_pcaps_allN.csv\"\n",
    "dga_malware_synthetic2 = \"/home/ubuntu/DoH_DGA_training/datasets/DGA_CF/all_pcaps_allN.csv\"\n",
    "\n",
    "nondoh_benign_file = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/all_nondoh.csv\"\n",
    "doh_benign_file = \"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/all_doh.csv\"\n",
    "\n",
    "# ===============================\n",
    "# Load datasets\n",
    "# ===============================\n",
    "df_dga_malware = pd.read_csv(dga_malware_file)\n",
    "\n",
    "df_dga_synthetic1 = pd.read_csv(dga_malware_synthetic1)\n",
    "df_dga_synthetic2 = pd.read_csv(dga_malware_synthetic2)\n",
    "df_dga_synthetic = pd.concat([df_dga_synthetic1, df_dga_synthetic2], ignore_index=True)\n",
    "\n",
    "df_doh_tunnel = pd.read_csv(doh_tunnel_file)\n",
    "df_nondoh_benign = pd.read_csv(nondoh_benign_file)\n",
    "df_doh_benign = pd.read_csv(doh_benign_file)\n",
    "\n",
    "# ===============================\n",
    "# Assign labels\n",
    "# ===============================\n",
    "df_dga_malware[\"label\"] = \"DGA_MALWARE\"\n",
    "df_dga_synthetic[\"label\"] = \"DGA_Synthetic\"\n",
    "df_doh_tunnel[\"label\"] = \"DoH_Tunnel\"\n",
    "df_nondoh_benign[\"label\"] = \"NonDoH_Benign\"\n",
    "df_doh_benign[\"label\"] = \"DoH_Benign\"\n",
    "\n",
    "# ===============================\n",
    "# Combine all into one DataFrame\n",
    "# ===============================\n",
    "df_all = pd.concat(\n",
    "    [df_dga_malware, df_dga_synthetic, df_doh_tunnel, df_nondoh_benign, df_doh_benign],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"DGA_MALWARE shape:\", df_dga_malware.shape)\n",
    "print(\"DGA_Synthetic shape:\", df_dga_synthetic.shape)\n",
    "print(\"DoH_Tunnel shape:\", df_doh_tunnel.shape)\n",
    "print(\"NonDoH_Benign shape:\", df_nondoh_benign.shape)\n",
    "print(\"DoH_Benign shape:\", df_doh_benign.shape)\n",
    "\n",
    "print(\"\\nFinal dataset shape:\", df_all.shape)\n",
    "print(\"Labels distribution:\\n\", df_all[\"label\"].value_counts())\n",
    "print(\"Columns:\\n\", list(df_all.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64c40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on N=16, dataset size=618516\n",
      "Remaining feature columns:\n",
      " ['n_client', 'client_bytes', 'client_pkt_min', 'client_pkt_mean', 'client_pkt_max', 'client_iat_min', 'client_iat_mean', 'client_iat_max', 'n_server', 'server_bytes', 'server_pkt_min', 'server_pkt_mean', 'server_pkt_max', 'server_iat_min', 'server_iat_mean', 'server_iat_max', 'pkt_fraction_client', 'bytes_fraction_client', 'flow_duration', 'time_first_response', 'dir_switches', 'size_min', 'size_mean', 'size_max', 'iat_min', 'iat_mean', 'iat_max']\n",
      "Target classes: ['DGA_MALWARE' 'DGA_Synthetic' 'DoH_Tunnel' 'NonDoH_Benign' 'DoH_Benign']\n"
     ]
    }
   ],
   "source": [
    "N_value = 16   # <-- change this to 8, 16, 32, 64 as needed\n",
    "df_subset = df_all[df_all[\"N\"] == N_value].copy()\n",
    "\n",
    "print(f\"\\nTraining on N={N_value}, dataset size={df_subset.shape[0]}\")\n",
    "\n",
    "# Separate features and target\n",
    "y = df_subset[\"label\"]\n",
    "X = df_subset.drop(columns=[\n",
    "    \"label\", 'session_id', 'client_ip', 'server_ip', \n",
    "    'client_port', 'server_port', 'protocol', 'N',\n",
    "    'pcap_file', 'source_pcap'\n",
    "], errors=\"ignore\")\n",
    "feature_names = X.columns.tolist()\n",
    "# Keep only numeric features\n",
    "X = X.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "\n",
    "print(\"Remaining feature columns:\\n\", list(X.columns))\n",
    "print(\"Target classes:\", y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302824e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Running experiments for N = 8 ========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size for N=8: 618516\n",
      "\n",
      "Stage A_DoH_vs_NonDoH (N=8) - samples: 618516, numeric features: 27\n",
      "Label distribution: Counter({'NonDoH': 587911, 'DoH': 30605})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=A_DoH_vs_NonDoH, N=8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DoH     0.9717    0.9822    0.9769      6121\n",
      "      NonDoH     0.9991    0.9985    0.9988    117583\n",
      "\n",
      "    accuracy                         0.9977    123704\n",
      "   macro avg     0.9854    0.9904    0.9879    123704\n",
      "weighted avg     0.9977    0.9977    0.9977    123704\n",
      "\n",
      "Confusion matrix:\n",
      " [[  6012    109]\n",
      " [   175 117408]]\n",
      "\n",
      "Stage B_DGA_vs_NonDGA (N=8) - samples: 587911, numeric features: 27\n",
      "Label distribution: Counter({'NonDGA_Benign': 534801, 'DGA': 53110})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=B_DGA_vs_NonDGA, N=8:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DGA     0.9992    0.9987    0.9990     10622\n",
      "NonDGA_Benign     0.9999    0.9999    0.9999    106961\n",
      "\n",
      "     accuracy                         0.9998    117583\n",
      "    macro avg     0.9996    0.9993    0.9994    117583\n",
      " weighted avg     0.9998    0.9998    0.9998    117583\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 10608     14]\n",
      " [     8 106953]]\n",
      "\n",
      "Stage C_DoH_Tunnel_vs_Benign (N=8) - samples: 30605, numeric features: 27\n",
      "Label distribution: Counter({'DoH_Benign': 29103, 'DoH_Tunnel': 1502})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=C_DoH_Tunnel_vs_Benign, N=8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DoH_Benign     0.9998    0.9993    0.9996      5821\n",
      "  DoH_Tunnel     0.9868    0.9967    0.9917       300\n",
      "\n",
      "    accuracy                         0.9992      6121\n",
      "   macro avg     0.9933    0.9980    0.9956      6121\n",
      "weighted avg     0.9992    0.9992    0.9992      6121\n",
      "\n",
      "Confusion matrix:\n",
      " [[5817    4]\n",
      " [   1  299]]\n",
      "\n",
      "\n",
      "======== Running experiments for N = 16 ========\n",
      "Dataset size for N=16: 618516\n",
      "\n",
      "Stage A_DoH_vs_NonDoH (N=16) - samples: 618516, numeric features: 27\n",
      "Label distribution: Counter({'NonDoH': 587911, 'DoH': 30605})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=A_DoH_vs_NonDoH, N=16:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DoH     0.9729    0.9915    0.9821      6121\n",
      "      NonDoH     0.9996    0.9986    0.9991    117583\n",
      "\n",
      "    accuracy                         0.9982    123704\n",
      "   macro avg     0.9862    0.9950    0.9906    123704\n",
      "weighted avg     0.9982    0.9982    0.9982    123704\n",
      "\n",
      "Confusion matrix:\n",
      " [[  6069     52]\n",
      " [   169 117414]]\n",
      "\n",
      "Stage B_DGA_vs_NonDGA (N=16) - samples: 587911, numeric features: 27\n",
      "Label distribution: Counter({'NonDGA_Benign': 534801, 'DGA': 53110})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=B_DGA_vs_NonDGA, N=16:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DGA     0.9999    0.9989    0.9994     10622\n",
      "NonDGA_Benign     0.9999    1.0000    0.9999    106961\n",
      "\n",
      "     accuracy                         0.9999    117583\n",
      "    macro avg     0.9999    0.9994    0.9997    117583\n",
      " weighted avg     0.9999    0.9999    0.9999    117583\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 10610     12]\n",
      " [     1 106960]]\n",
      "\n",
      "Stage C_DoH_Tunnel_vs_Benign (N=16) - samples: 30605, numeric features: 27\n",
      "Label distribution: Counter({'DoH_Benign': 29103, 'DoH_Tunnel': 1502})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=C_DoH_Tunnel_vs_Benign, N=16:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DoH_Benign     0.9993    0.9998    0.9996      5821\n",
      "  DoH_Tunnel     0.9966    0.9867    0.9916       300\n",
      "\n",
      "    accuracy                         0.9992      6121\n",
      "   macro avg     0.9980    0.9932    0.9956      6121\n",
      "weighted avg     0.9992    0.9992    0.9992      6121\n",
      "\n",
      "Confusion matrix:\n",
      " [[5820    1]\n",
      " [   4  296]]\n",
      "\n",
      "\n",
      "======== Running experiments for N = 32 ========\n",
      "Dataset size for N=32: 618516\n",
      "\n",
      "Stage A_DoH_vs_NonDoH (N=32) - samples: 618516, numeric features: 27\n",
      "Label distribution: Counter({'NonDoH': 587911, 'DoH': 30605})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=A_DoH_vs_NonDoH, N=32:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DoH     0.9761    0.9933    0.9846      6121\n",
      "      NonDoH     0.9997    0.9987    0.9992    117583\n",
      "\n",
      "    accuracy                         0.9985    123704\n",
      "   macro avg     0.9879    0.9960    0.9919    123704\n",
      "weighted avg     0.9985    0.9985    0.9985    123704\n",
      "\n",
      "Confusion matrix:\n",
      " [[  6080     41]\n",
      " [   149 117434]]\n",
      "\n",
      "Stage B_DGA_vs_NonDGA (N=32) - samples: 587911, numeric features: 27\n",
      "Label distribution: Counter({'NonDGA_Benign': 534801, 'DGA': 53110})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=B_DGA_vs_NonDGA, N=32:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DGA     0.9999    0.9990    0.9994     10622\n",
      "NonDGA_Benign     0.9999    1.0000    0.9999    106961\n",
      "\n",
      "     accuracy                         0.9999    117583\n",
      "    macro avg     0.9999    0.9995    0.9997    117583\n",
      " weighted avg     0.9999    0.9999    0.9999    117583\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 10611     11]\n",
      " [     1 106960]]\n",
      "\n",
      "Stage C_DoH_Tunnel_vs_Benign (N=32) - samples: 30605, numeric features: 27\n",
      "Label distribution: Counter({'DoH_Benign': 29103, 'DoH_Tunnel': 1502})\n",
      "Training MLP...\n",
      "\n",
      "Classification report for Stage=C_DoH_Tunnel_vs_Benign, N=32:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DoH_Benign     1.0000    0.9998    0.9999      5821\n",
      "  DoH_Tunnel     0.9967    1.0000    0.9983       300\n",
      "\n",
      "    accuracy                         0.9998      6121\n",
      "   macro avg     0.9983    0.9999    0.9991      6121\n",
      "weighted avg     0.9998    0.9998    0.9998      6121\n",
      "\n",
      "Confusion matrix:\n",
      " [[5820    1]\n",
      " [   0  300]]\n",
      "\n",
      "\n",
      "Summary results (first rows):\n",
      "   N                   stage          class  precision    recall        f1  \\\n",
      "0  8         A_DoH_vs_NonDoH            DoH   0.971715  0.982192  0.976926   \n",
      "1  8         A_DoH_vs_NonDoH         NonDoH   0.999072  0.998512  0.998792   \n",
      "2  8         B_DGA_vs_NonDGA            DGA   0.999246  0.998682  0.998964   \n",
      "3  8         B_DGA_vs_NonDGA  NonDGA_Benign   0.999869  0.999925  0.999897   \n",
      "4  8  C_DoH_Tunnel_vs_Benign     DoH_Benign   0.999828  0.999313  0.999570   \n",
      "\n",
      "    support  \n",
      "0    6121.0  \n",
      "1  117583.0  \n",
      "2   10622.0  \n",
      "3  106961.0  \n",
      "4    5821.0  \n",
      "\n",
      "Saved results to hierarchical_mlp_results_by_N_and_stage.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# ---- Config ----\n",
    "N_values = [8, 16, 32, 64]      # run for each N\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "# MLP config (match your reduced model)\n",
    "mlp_params = dict(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-4,\n",
    "    batch_size=64,\n",
    "    learning_rate=\"adaptive\",\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    shuffle=True,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=random_state,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# columns to drop from features (identifiers etc)\n",
    "DROP_COLS = [\n",
    "    \"label\", \"session_id\", \"client_ip\", \"server_ip\",\n",
    "    \"client_port\", \"server_port\", \"protocol\", \"N\",\n",
    "    \"pcap_file\", \"source_pcap\"\n",
    "]\n",
    "\n",
    "# ---- Functions to prepare datasets for each stage ----\n",
    "def make_stage_A(df):\n",
    "    \"\"\"Stage A: DoH vs NonDoH\"\"\"\n",
    "    df_stage = df.copy()\n",
    "    df_stage = df_stage[df_stage[\"label\"].isin([\n",
    "        \"DoH_Benign\", \"DoH_Tunnel\", \"NonDoH_Benign\", \"DGA_Synthetic\", \"DGA_MALWARE\"\n",
    "    ])]\n",
    "    df_stage[\"stage_label\"] = df_stage[\"label\"].apply(lambda x: \"DoH\" if x.startswith(\"DoH_\") else \"NonDoH\")\n",
    "    return df_stage\n",
    "\n",
    "def make_stage_B(df):\n",
    "    \"\"\"Stage B: within NonDoH: DGA vs NonDoH_Benign\n",
    "       We keep only rows that are in NonDoH_Benign or DGA_*\"\"\"\n",
    "    df_stage = df[df[\"label\"].isin([\"NonDoH_Benign\", \"DGA_Synthetic\", \"DGA_MALWARE\"])].copy()\n",
    "    df_stage[\"stage_label\"] = df_stage[\"label\"].apply(lambda x: \"DGA\" if x.startswith(\"DGA_\") else \"NonDGA_Benign\")\n",
    "    return df_stage\n",
    "\n",
    "def make_stage_C(df):\n",
    "    \"\"\"Stage C: within DoH: DoH_Tunnel vs DoH_Benign\"\"\"\n",
    "    df_stage = df[df[\"label\"].isin([\"DoH_Benign\", \"DoH_Tunnel\"])].copy()\n",
    "    df_stage[\"stage_label\"] = df_stage[\"label\"].apply(lambda x: \"DoH_Tunnel\" if x == \"DoH_Tunnel\" else \"DoH_Benign\")\n",
    "    return df_stage\n",
    "\n",
    "STAGES = {\n",
    "    \"A_DoH_vs_NonDoH\": make_stage_A,\n",
    "    \"B_DGA_vs_NonDGA\": make_stage_B,\n",
    "    \"C_DoH_Tunnel_vs_Benign\": make_stage_C\n",
    "}\n",
    "\n",
    "# ---- storage for results ----\n",
    "results = []\n",
    "\n",
    "# ---- main loop ----\n",
    "for N in N_values:\n",
    "    print(f\"\\n\\n======== Running experiments for N = {N} ========\")\n",
    "    df_subset = df_all[df_all[\"N\"] == N].copy()\n",
    "    print(f\"Dataset size for N={N}: {df_subset.shape[0]}\")\n",
    "    if df_subset.shape[0] == 0:\n",
    "        print(\"No rows for this N, skipping.\")\n",
    "        continue\n",
    "\n",
    "    for stage_name, stage_fn in STAGES.items():\n",
    "        df_stage = stage_fn(df_subset)\n",
    "        if df_stage.shape[0] == 0:\n",
    "            print(f\"Stage {stage_name}: no data, skipping\")\n",
    "            continue\n",
    "\n",
    "        # keep only rows with non-null stage_label\n",
    "        df_stage = df_stage.dropna(subset=[\"stage_label\"])\n",
    "        # features and target\n",
    "        y = df_stage[\"stage_label\"].copy()\n",
    "        X = df_stage.drop(columns=DROP_COLS, errors=\"ignore\").copy()\n",
    "\n",
    "        # keep numeric columns only (you already requested this)\n",
    "        X = X.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "        print(f\"\\nStage {stage_name} (N={N}) - samples: {X.shape[0]}, numeric features: {X.shape[1]}\")\n",
    "        print(\"Label distribution:\", Counter(y))\n",
    "\n",
    "        # ensure we have at least two classes in this stage\n",
    "        if len(y.unique()) < 2:\n",
    "            print(f\"Stage {stage_name} (N={N}) has <2 classes, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # encode labels to 0/1\n",
    "        le = LabelEncoder()\n",
    "        y_enc = le.fit_transform(y)\n",
    "\n",
    "        # train/test split with stratify\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_enc, test_size=test_size, stratify=y_enc, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # pipeline: imputer -> scaler -> MLP\n",
    "        pipeline = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"mlp\", MLPClassifier(**mlp_params))\n",
    "        ])\n",
    "\n",
    "        # Train\n",
    "        print(\"Training MLP...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predict + report\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        class_names = le.inverse_transform([0,1]) if len(le.classes_)==2 else le.classes_\n",
    "        print(f\"\\nClassification report for Stage={stage_name}, N={N}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_, digits=4))\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # store summary metrics (you can expand with more metrics)\n",
    "        # compute per-class support, f1, precision, recall via classification_report dict\n",
    "        report_dict = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)\n",
    "        for cls in le.classes_:\n",
    "            entry = {\n",
    "                \"N\": N,\n",
    "                \"stage\": stage_name,\n",
    "                \"class\": cls,\n",
    "                \"precision\": report_dict[cls][\"precision\"],\n",
    "                \"recall\": report_dict[cls][\"recall\"],\n",
    "                \"f1\": report_dict[cls][\"f1-score\"],\n",
    "                \"support\": report_dict[cls][\"support\"]\n",
    "            }\n",
    "            results.append(entry)\n",
    "\n",
    "# ---- collect results to DataFrame ----\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\nSummary results (first rows):\")\n",
    "print(results_df.head())\n",
    "\n",
    "# optionally save to csv\n",
    "#results_df.to_csv(\"hierarchical_mlp_results_by_N_and_stage.csv\", index=False)\n",
    "print(\"\\nSaved results to hierarchical_mlp_results_by_N_and_stage.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52479a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
