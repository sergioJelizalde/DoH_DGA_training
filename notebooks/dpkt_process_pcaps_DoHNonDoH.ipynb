{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06f6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpkt\n",
    "import socket\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a292da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PCAP folders to process\n",
    "pcap_folders = [\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-AdGuard/AdGuard\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Cloudflare/Cloudflare\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Google/Google\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-AdGuard/AdGuard/1\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-AdGuard/AdGuard/2\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-CloudFlare/CloudFlare/1\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-CloudFlare/CloudFlare/2\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-Google/Google/large\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-Quad9/Quad9/1\",\n",
    "    r\"/home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-Quad9/Quad9/2\",\n",
    "]\n",
    "\n",
    "# List of benign DoH IPs\n",
    "doh_ips = [\n",
    "    \"1.1.1.1\",\n",
    "    \"8.8.4.4\",\n",
    "    \"8.8.8.8\",\n",
    "    \"9.9.9.9\",\n",
    "    \"9.9.9.10\",\n",
    "    \"9.9.9.11\",\n",
    "    \"176.103.130.131\",\n",
    "    \"176.103.130.130\",\n",
    "    \"149.112.112.10\",\n",
    "    \"149.112.112.112\",\n",
    "    \"104.16.248.249\",\n",
    "    \"104.16.249.249\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989c3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inet_to_str(inet):\n",
    "    try:\n",
    "        return socket.inet_ntop(socket.AF_INET, inet)\n",
    "    except ValueError:\n",
    "        return socket.inet_ntop(socket.AF_INET6, inet)\n",
    "\n",
    "def normalize_session(src, dst, sport, dport, proto):\n",
    "    \"\"\"\n",
    "    Define session by (client_ip, server_ip, client_port, server_port, proto).\n",
    "    Client is assumed to be the side with port != 443, server is port 443.\n",
    "    \"\"\"\n",
    "    if dport == 443:\n",
    "        client, server = (src, sport), (dst, dport)\n",
    "    elif sport == 443:\n",
    "        client, server = (dst, dport), (src, sport)\n",
    "    else:\n",
    "        # not a TLS session, skip\n",
    "        return None\n",
    "    return (client[0], server[0], client[1], server[1], proto)\n",
    "\n",
    "def process_pcap_tls(pcap_file, N_values=[8,16,32,64], max_packets=None):\n",
    "    sessions = defaultdict(lambda: {\n",
    "        \"client_ts\": [], \"client_sizes\": [],\n",
    "        \"server_ts\": [], \"server_sizes\": [],\n",
    "        \"appdata_count\": 0\n",
    "    })\n",
    "\n",
    "    with open(pcap_file, \"rb\") as f:\n",
    "        pcap = dpkt.pcap.Reader(f)\n",
    "\n",
    "        for i, (ts, buf) in enumerate(pcap):\n",
    "            try:\n",
    "                eth = dpkt.ethernet.Ethernet(buf)\n",
    "                if not isinstance(eth.data, dpkt.ip.IP):\n",
    "                    continue\n",
    "                ip = eth.data\n",
    "                l4 = ip.data\n",
    "                if not isinstance(l4, dpkt.tcp.TCP):\n",
    "                    continue\n",
    "\n",
    "                src = inet_to_str(ip.src)\n",
    "                dst = inet_to_str(ip.dst)\n",
    "                sport, dport = l4.sport, l4.dport\n",
    "                fid = normalize_session(src, dst, sport, dport, \"TCP\")\n",
    "                if fid is None:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    records, _ = dpkt.ssl.tls_multi_factory(l4.data)\n",
    "                except (dpkt.ssl.SSL3Exception, dpkt.dpkt.NeedData):\n",
    "                    continue\n",
    "\n",
    "                for rec in records:\n",
    "                    if rec.type == 23:  # TLS App Data\n",
    "                        if dport == 443:  # client → server\n",
    "                            sessions[fid][\"client_ts\"].append(float(ts))\n",
    "                            sessions[fid][\"client_sizes\"].append(len(rec.data))\n",
    "                        elif sport == 443:  # server → client\n",
    "                            sessions[fid][\"server_ts\"].append(float(ts))\n",
    "                            sessions[fid][\"server_sizes\"].append(len(rec.data))\n",
    "\n",
    "                        sessions[fid][\"appdata_count\"] += 1\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if max_packets and i > max_packets:\n",
    "                break\n",
    "\n",
    "    results = {N: [] for N in N_values}\n",
    "    for fid, stats in sessions.items():\n",
    "        combined = []\n",
    "        combined += [(t, \"client\", s) for t, s in zip(stats[\"client_ts\"], stats[\"client_sizes\"])]\n",
    "        combined += [(t, \"server\", s) for t, s in zip(stats[\"server_ts\"], stats[\"server_sizes\"])]\n",
    "        combined.sort(key=lambda x: x[0])\n",
    "\n",
    "        for N in N_values:\n",
    "            subset = combined[:N]\n",
    "            if not subset:\n",
    "                continue\n",
    "\n",
    "            client_sizes = [s for _, d, s in subset if d == \"client\"]\n",
    "            server_sizes = [s for _, d, s in subset if d == \"server\"]\n",
    "            client_ts = [t for t, d, _ in subset if d == \"client\"]\n",
    "            server_ts = [t for t, d, _ in subset if d == \"server\"]\n",
    "\n",
    "            client_iats = np.diff(sorted(client_ts)) if len(client_ts) > 1 else []\n",
    "            server_iats = np.diff(sorted(server_ts)) if len(server_ts) > 1 else []\n",
    "\n",
    "            # ---- New features ----\n",
    "            n_client, n_server = len(client_sizes), len(server_sizes)\n",
    "            client_bytes, server_bytes = np.sum(client_sizes), np.sum(server_sizes)\n",
    "\n",
    "            pkt_fraction_client = n_client / (n_client + n_server) if (n_client + n_server) > 0 else 0.0\n",
    "            bytes_fraction_client = client_bytes / (client_bytes + server_bytes) if (client_bytes + server_bytes) > 0 else 0.0\n",
    "\n",
    "            # direction switches\n",
    "            dirs = [d for _, d, _ in subset]\n",
    "            dir_switches = sum(1 for i in range(1, len(dirs)) if dirs[i] != dirs[i-1])\n",
    "\n",
    "            # flow duration\n",
    "            flow_duration = subset[-1][0] - subset[0][0] if len(subset) > 1 else 0.0\n",
    "\n",
    "            # time to first response\n",
    "            if client_ts and server_ts:\n",
    "                first_client = min(client_ts)\n",
    "                first_server = min(server_ts)\n",
    "                time_first_response = first_server - first_client if first_server > first_client else 0.0\n",
    "            else:\n",
    "                time_first_response = 0.0\n",
    "\n",
    "            # global packet size stats\n",
    "            all_sizes = [s for _, _, s in subset]\n",
    "            size_min = np.min(all_sizes) if all_sizes else 0\n",
    "            size_mean = np.mean(all_sizes) if all_sizes else 0.0\n",
    "            size_max = np.max(all_sizes) if all_sizes else 0\n",
    "\n",
    "            # global IAT stats (all packets regardless of dir)\n",
    "            all_ts = [t for t, _, _ in subset]\n",
    "            all_iats = np.diff(sorted(all_ts)) if len(all_ts) > 1 else []\n",
    "            iat_min = np.min(all_iats) if len(all_iats) else 0.0\n",
    "            iat_mean = np.mean(all_iats) if len(all_iats) else 0.0\n",
    "            iat_max = np.max(all_iats) if len(all_iats) else 0.0\n",
    "            # ----------------------\n",
    "\n",
    "            records = {\n",
    "                \"session_id\": fid,\n",
    "                \"client_ip\": fid[0],\n",
    "                \"server_ip\": fid[1],\n",
    "                \"client_port\": fid[2],\n",
    "                \"server_port\": fid[3],\n",
    "                \"protocol\": fid[4],\n",
    "                \"N\": N,\n",
    "\n",
    "                \"n_client\": n_client,\n",
    "                \"client_bytes\": int(client_bytes),\n",
    "                \"client_pkt_min\": int(np.min(client_sizes)) if client_sizes else 0,\n",
    "                \"client_pkt_mean\": float(np.mean(client_sizes)) if client_sizes else 0.0,\n",
    "                \"client_pkt_max\": int(np.max(client_sizes)) if client_sizes else 0,\n",
    "                \"client_iat_min\": float(np.min(client_iats)) if len(client_iats) else 0.0,\n",
    "                \"client_iat_mean\": float(np.mean(client_iats)) if len(client_iats) else 0.0,\n",
    "                \"client_iat_max\": float(np.max(client_iats)) if len(client_iats) else 0.0,\n",
    "\n",
    "                \"n_server\": n_server,\n",
    "                \"server_bytes\": int(server_bytes),\n",
    "                \"server_pkt_min\": int(np.min(server_sizes)) if server_sizes else 0,\n",
    "                \"server_pkt_mean\": float(np.mean(server_sizes)) if server_sizes else 0.0,\n",
    "                \"server_pkt_max\": int(np.max(server_sizes)) if server_sizes else 0,\n",
    "                \"server_iat_min\": float(np.min(server_iats)) if len(server_iats) else 0.0,\n",
    "                \"server_iat_mean\": float(np.mean(server_iats)) if len(server_iats) else 0.0,\n",
    "                \"server_iat_max\": float(np.max(server_iats)) if len(server_iats) else 0.0,\n",
    "\n",
    "                # new flow-level ratios\n",
    "                \"pkt_fraction_client\": pkt_fraction_client,\n",
    "                \"bytes_fraction_client\": bytes_fraction_client,\n",
    "\n",
    "                # new flow-level timing\n",
    "                \"flow_duration\": flow_duration,\n",
    "                \"time_first_response\": time_first_response,\n",
    "\n",
    "                # new flow-level directionality\n",
    "                \"dir_switches\": dir_switches,\n",
    "\n",
    "                # new global stats\n",
    "                \"size_min\": int(size_min),\n",
    "                \"size_mean\": float(size_mean),\n",
    "                \"size_max\": int(size_max),\n",
    "                \"iat_min\": float(iat_min),\n",
    "                \"iat_mean\": float(iat_mean),\n",
    "                \"iat_max\": float(iat_max),\n",
    "            }\n",
    "\n",
    "            results[N].append(records)\n",
    "\n",
    "    for N in results:\n",
    "        results[N] = pd.DataFrame(results[N])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6aff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-AdGuard/AdGuard/dump_00004_20200114141606.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-AdGuard/AdGuard/dump_00002_20200114114901.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-AdGuard/AdGuard/dump_00005_20200114153502.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-AdGuard/AdGuard/dump_00003_20200114130936.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-AdGuard/AdGuard/dump_00001_20200114102945.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Cloudflare/Cloudflare/dump_00004_20200113193921.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Cloudflare/Cloudflare/dump_00005_20200113205730.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Cloudflare/Cloudflare/dump_00001_20200113152847.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Cloudflare/Cloudflare/dump_00002_20200113162614.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Cloudflare/Cloudflare/dump_00003_20200113182754.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Google/Google/dump_00001_20200113100617.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Google/Google/dump_00003_20200113120939.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Google/Google/dump_00002_20200113111300.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Google/Google/dump_00005_20200113142226.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Google/Google/dump_00004_20200113132407.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00002_20200111232233.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00009_20200113074803.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00006_20200112040340.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00004_20200112015042.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00003_20200112004539.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00008_20200112063544.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00001_20200111222621.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00005_20200112025337.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Chrome-Quad9/Quad9/dump_00007_20200112051950.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-AdGuard/AdGuard/1/dump.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-AdGuard/AdGuard/2/dump.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-CloudFlare/CloudFlare/1/dump.pcap...\n",
      "[-] Error processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-CloudFlare/CloudFlare/1/dump.pcap: got 5, 16 needed at least\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-CloudFlare/CloudFlare/2/dump.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-Google/Google/large/dump.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-Quad9/Quad9/1/dump.pcap...\n",
      "[+] Processing /home/ubuntu/DoH_DGA_training/datasets/PCAPs/DoHBenign-NonDoH/BenignDoH_NonDoH-Firefox-Quad9/Quad9/2/dump.pcap...\n",
      "[+] Saved all_doh.csv with 116412 rows\n",
      "[+] Saved all_nondoh.csv with 2139204 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_doh_csv = \"all_doh.csv\"\n",
    "output_nondoh_csv = \"all_nondoh.csv\"\n",
    "\n",
    "# === PROCESS ===\n",
    "all_doh = []\n",
    "all_nondoh = []\n",
    "\n",
    "for folder in pcap_folders:\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.endswith(\".pcap\"):\n",
    "            continue\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        print(f\"[+] Processing {fpath}...\")\n",
    "\n",
    "        try:\n",
    "            results = process_pcap_tls(fpath)  # uses your updated feature extractor\n",
    "\n",
    "            # merge all N into a single DataFrame\n",
    "            df_allN = pd.concat(results.values(), ignore_index=True)\n",
    "\n",
    "            # classify by server IP\n",
    "            mask_doh = df_allN[\"server_ip\"].isin(doh_ips)\n",
    "            df_doh = df_allN[mask_doh].copy()\n",
    "            df_nondoh = df_allN[~mask_doh].copy()\n",
    "\n",
    "            # add file origin (optional)\n",
    "            df_doh[\"source_pcap\"] = fname\n",
    "            df_nondoh[\"source_pcap\"] = fname\n",
    "\n",
    "            all_doh.append(df_doh)\n",
    "            all_nondoh.append(df_nondoh)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[-] Error processing {fpath}: {e}\")\n",
    "\n",
    "# combine across all pcaps\n",
    "df_doh_all = pd.concat(all_doh, ignore_index=True) if all_doh else pd.DataFrame()\n",
    "df_nondoh_all = pd.concat(all_nondoh, ignore_index=True) if all_nondoh else pd.DataFrame()\n",
    "\n",
    "# save to CSV\n",
    "df_doh_all.to_csv(output_doh_csv, index=False)\n",
    "df_nondoh_all.to_csv(output_nondoh_csv, index=False)\n",
    "\n",
    "print(f\"[+] Saved {output_doh_csv} with {len(df_doh_all)} rows\")\n",
    "print(f\"[+] Saved {output_nondoh_csv} with {len(df_nondoh_all)} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
